{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"projectprofe.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Final Project: Image super resolution**"]},{"cell_type":"markdown","metadata":{"id":"TPhaB5uN5nub"},"source":["## ESRGAN definition"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZF3oyes121H4","executionInfo":{"status":"ok","timestamp":1624198112582,"user_tz":-120,"elapsed":54337,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}},"outputId":"2403189b-f855-429b-d2f0-7d901c109127"},"source":["## Create a Custom Dataset for CK database\n","import scipy.io as sio\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import torch.nn.functional as F\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","data_path = '/content/drive/Shareddrives/DeepLearning/DeepLearning_2021/finalProject/Data/'\n","results_path = '/content/drive/Shareddrives/DeepLearning/DeepLearning_2021/finalProject/Results/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ixa7eudJhFma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624198196117,"user_tz":-120,"elapsed":64194,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}},"outputId":"d28bda8c-2088-409b-b963-b7b3cc5945be"},"source":["STL = torchvision.datasets.STL10(data_path, transform=tf.ToTensor(), download=True, split='train')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Co3vuWnSROP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624198236584,"user_tz":-120,"elapsed":10505,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}},"outputId":"631f9a34-354b-437e-9279-2445f68a76f0"},"source":["STLTest = torchvision.datasets.STL10(data_path, transform=tf.ToTensor(), download=True, split='test')\n","  "],"execution_count":3,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ryAX_FffsKfi","executionInfo":{"status":"ok","timestamp":1624198241299,"user_tz":-120,"elapsed":256,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}}},"source":["train_loader = torch.utils.data.DataLoader(dataset=STL,\n","                                           batch_size=128, \n","                                           shuffle=True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmrp8uBAB74d","executionInfo":{"status":"ok","timestamp":1624198248470,"user_tz":-120,"elapsed":357,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}}},"source":["import torch\n","import torch.nn as nn\n","\n","class RDB(nn.Module):\n","  def __init__(self):\n","    super(RDB, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(64,32,kernel_size=3, stride=1, padding = 1)\n","    self.conv2 = nn.Conv2d(96,32,kernel_size=3, stride=1, padding = 1)\n","    self.conv3 = nn.Conv2d(128,32,kernel_size=3, stride=1, padding = 1)\n","    self.conv4 = nn.Conv2d(160,32,kernel_size=3, stride=1, padding = 1)\n","    self.conv5 = nn.Conv2d(192,64,kernel_size=3, stride=1, padding = 1)\n","\n","    self.leakyRelu = nn.LeakyReLU(inplace=True, negative_slope = 0.2)\n","\n","  def forward(self,x):\n","    out1 = self.leakyRelu(self.conv1(x))\n","    out2 = self.leakyRelu(self.conv2(torch.cat((x, out1), 1)))\n","    out3 = self.leakyRelu(self.conv3(torch.cat((x, out1, out2), 1)))\n","    out4 = self.leakyRelu(self.conv4(torch.cat((x, out1, out2, out3), 1)))\n","    out = self.conv5(torch.cat((x, out1, out2, out3, out4), 1))\n","\n","    return out\n","\n","class RRDB(nn.Module):\n","  def __init__(self, scalingParam = 0.5):\n","    super(RRDB, self).__init__()\n","    self.scalingParam = scalingParam\n","    self.RDB1 = RDB()\n","    self.RDB2 = RDB()\n","    self.RDB3 = RDB()\n","\n","  def forward(self,x):\n","    out1 = self.RDB1(x)\n","    out = x + out1 * self.scalingParam\n","    out2 = self.RDB2(out)\n","    out = out + out2 * self.scalingParam\n","    out3 = self.RDB3(out)\n","    out = out + out3 * self.scalingParam\n","\n","    return out\n","\n","class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    self.layer1 = nn.Conv2d(3,64,kernel_size=3, stride=1, padding = 1)\n","\n","    self.RRDB1 = RRDB()\n","    self.RRDB2 = RRDB()\n","    self.RRDB3 = RRDB()    \n","\n","    self.pool = nn.UpsamplingNearest2d(scale_factor=3)\n","\n","    self.conv1 = nn.Conv2d(128,64,kernel_size=3, stride=1, padding = 1)\n","    self.conv2 = nn.Conv2d(64,64,kernel_size=3, stride=1, padding = 1)\n","    self.conv3 = nn.Conv2d(64,3,kernel_size=3, stride=1, padding = 1)\n","\n","    self.leakyRelu = nn.LeakyReLU(inplace=True, negative_slope = 0.2)\n","  \n","  def forward(self,x):\n","\n","    out = self.layer1(x)\n","\n","    out1 = self.RRDB1(out)\n","    out1 = self.RRDB2(out1)\n","    out1 = self.RRDB3(out1)\n","\n","    out = torch.cat((out, out1), 1)\n","\n","    out = self.pool(out)\n","\n","    out = self.leakyRelu(self.conv1(out))\n","    out = self.leakyRelu(self.conv2(out))\n","    out = self.leakyRelu(self.conv3(out))\n","\n","    return out\n","\n","  # Sample a set of images\n","  def sample(self, realImage):\n","    tr = F.interpolate(realImage, size=32)\n","    return tr\n","\n","\n","# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#fer el decoder com el del paper\n","class Discriminator(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Discriminator, self).__init__()\n","    self.layer1 = ConvBNReLU(3,base_channels,pooling=True)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(12*12*base_channels*4,out_features)\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    out = self.fc(out.view(x.shape[0],-1))\n","    return torch.sigmoid(out)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHAPvcRHCHHk","executionInfo":{"status":"ok","timestamp":1624198257351,"user_tz":-120,"elapsed":2342,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}}},"source":["# GAN Train function. We have a generator and discriminator models and their respective optimizers.\n","def train_GAN(gen, disc,  train_loader, optimizer_gen, optim_disc,\n","              num_epochs=10, model_name='gan_ESRGAN.ckpt', device='cpu', gan_weight = 0.1):\n","    gen = gen.to(device)\n","    gen.train() # Set the generator in train mode\n","    disc = disc.to(device)\n","    disc.train() # Set the discriminator in train mode\n","\n","    total_step = len(train_loader)\n","    losses_list = []\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        disc_loss_avg = 0\n","        gen_loss_avg = 0\n","        l1_loss_avg = 0\n","        nBatches = 0\n","        update_generator = True\n","\n","        for i, (real_images,_) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            real_images = real_images.to(device)\n","            n_images = real_images.shape[0]\n","\n","            # Forward pass\n","            # Generate images with the generator\n","            fake_images = gen.forward(gen.sample(real_images))\n","            \n","            # Use the discriminator to obtain the probabilties for real and generated images\n","            prob_real = disc(real_images)\n","            prob_fake = disc(fake_images)\n","            \n","            # Generator loss\n","            l1_loss = (real_images-fake_images).abs().mean()\n","\n","            # Generator loss\n","            gen_loss = -torch.log(1-prob_real).mean() - torch.log(prob_fake).mean()\n","   \n","            # Discriminator loss\n","            disc_loss = -torch.log(prob_real).mean() - torch.log(1-prob_fake).mean()\n","\n","            # We are going to update the discriminator and generator parameters alternatively at each iteration\n","\n","            if (update_generator):\n","              # Optimize generator\n","              # Backward and optimize\n","              optimizer_gen.zero_grad() # \n","              (gan_weight*gen_loss+l1_loss).backward() # Necessary to not erase intermediate variables needed for computing disc_loss gradient\n","              optimizer_gen.step()\n","              update_generator = False\n","            else:           \n","              # Optimize discriminator\n","              # Backward and optimize\n","              optimizer_disc.zero_grad()\n","              (disc_loss).backward()\n","              optimizer_disc.step()\n","              update_generator = True\n","\n","            disc_loss_avg += disc_loss.cpu().item()\n","            gen_loss_avg += gen_loss.cpu().item()\n","            \n","            l1_loss_avg += l1_loss.cpu().item()\n","            nBatches+=1\n","            if (i+1) % 20 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}, L1 Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches,  l1_loss_avg / nBatches))\n","            \n","        print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}, L1 Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches,  l1_loss_avg / nBatches))\n","        # Save model\n","        losses_list.append(disc_loss_avg / nBatches)\n","        torch.save(gan_gen.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqnESvFuC3n8","executionInfo":{"status":"ok","timestamp":1624203002204,"user_tz":-120,"elapsed":4742005,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}},"outputId":"ff426b74-dffb-4815-be28-ce5067a60af2"},"source":["# Define Generator and Discriminator networks\n","gan_gen = Generator()\n","gan_disc = Discriminator(1)\n","\n","#Initialize indepdent optimizer for both networks\n","learning_rate = .0005\n","optimizer_gen = torch.optim.Adam(gan_gen.parameters(),lr = learning_rate, weight_decay=1e-5)\n","optimizer_disc = torch.optim.Adam(gan_disc.parameters(),lr = learning_rate, weight_decay=1e-5)\n","\n","# Train the GAN\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","loss_list = train_GAN(gan_gen,gan_disc, train_loader, optimizer_gen, optimizer_disc,\n","                      num_epochs=50, model_name='gan_ESRGAN.ckpt', device=device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch [1/50], Step [40/40], Gen. Loss: 1.7343, Disc Loss: 1.2924, L1 Loss: 0.1916\n","Epoch [2/50], Step [40/40], Gen. Loss: 1.4071, Disc Loss: 1.5525, L1 Loss: 0.0950\n","Epoch [3/50], Step [40/40], Gen. Loss: 1.3139, Disc Loss: 1.6424, L1 Loss: 0.0903\n","Epoch [4/50], Step [40/40], Gen. Loss: 1.4034, Disc Loss: 1.5182, L1 Loss: 0.0793\n","Epoch [5/50], Step [40/40], Gen. Loss: 1.3821, Disc Loss: 1.4573, L1 Loss: 0.0710\n","Epoch [6/50], Step [40/40], Gen. Loss: 1.4216, Disc Loss: 1.3857, L1 Loss: 0.0593\n","Epoch [7/50], Step [40/40], Gen. Loss: 1.3988, Disc Loss: 1.3962, L1 Loss: 0.0581\n","Epoch [8/50], Step [40/40], Gen. Loss: 1.4094, Disc Loss: 1.3818, L1 Loss: 0.0556\n","Epoch [9/50], Step [40/40], Gen. Loss: 1.4049, Disc Loss: 1.3910, L1 Loss: 0.0539\n","Epoch [10/50], Step [40/40], Gen. Loss: 1.4047, Disc Loss: 1.3829, L1 Loss: 0.0520\n","Epoch [11/50], Step [40/40], Gen. Loss: 1.4093, Disc Loss: 1.3770, L1 Loss: 0.0522\n","Epoch [12/50], Step [40/40], Gen. Loss: 1.4228, Disc Loss: 1.3670, L1 Loss: 0.0509\n","Epoch [13/50], Step [40/40], Gen. Loss: 1.4290, Disc Loss: 1.3624, L1 Loss: 0.0510\n","Epoch [14/50], Step [40/40], Gen. Loss: 1.4271, Disc Loss: 1.3685, L1 Loss: 0.0533\n","Epoch [15/50], Step [40/40], Gen. Loss: 1.4191, Disc Loss: 1.3861, L1 Loss: 0.0548\n","Epoch [16/50], Step [40/40], Gen. Loss: 1.3752, Disc Loss: 1.4313, L1 Loss: 0.0572\n","Epoch [17/50], Step [40/40], Gen. Loss: 1.4231, Disc Loss: 1.3732, L1 Loss: 0.0528\n","Epoch [18/50], Step [40/40], Gen. Loss: 1.4183, Disc Loss: 1.3782, L1 Loss: 0.0521\n","Epoch [19/50], Step [40/40], Gen. Loss: 1.4185, Disc Loss: 1.3845, L1 Loss: 0.0534\n","Epoch [20/50], Step [40/40], Gen. Loss: 1.4214, Disc Loss: 1.3766, L1 Loss: 0.0532\n","Epoch [21/50], Step [40/40], Gen. Loss: 1.4322, Disc Loss: 1.3731, L1 Loss: 0.0538\n","Epoch [22/50], Step [40/40], Gen. Loss: 1.4037, Disc Loss: 1.4079, L1 Loss: 0.0567\n","Epoch [23/50], Step [40/40], Gen. Loss: 1.4266, Disc Loss: 1.3805, L1 Loss: 0.0550\n","Epoch [24/50], Step [40/40], Gen. Loss: 1.4425, Disc Loss: 1.3738, L1 Loss: 0.0564\n","Epoch [25/50], Step [40/40], Gen. Loss: 1.4056, Disc Loss: 1.3987, L1 Loss: 0.0578\n","Epoch [26/50], Step [40/40], Gen. Loss: 1.4333, Disc Loss: 1.3674, L1 Loss: 0.0546\n","Epoch [27/50], Step [40/40], Gen. Loss: 1.4212, Disc Loss: 1.3827, L1 Loss: 0.0556\n","Epoch [28/50], Step [40/40], Gen. Loss: 1.4245, Disc Loss: 1.3913, L1 Loss: 0.0571\n","Epoch [29/50], Step [40/40], Gen. Loss: 1.4364, Disc Loss: 1.3713, L1 Loss: 0.0575\n","Epoch [30/50], Step [40/40], Gen. Loss: 1.4386, Disc Loss: 1.3738, L1 Loss: 0.0564\n","Epoch [31/50], Step [40/40], Gen. Loss: 1.4366, Disc Loss: 1.3818, L1 Loss: 0.0603\n","Epoch [32/50], Step [40/40], Gen. Loss: 1.3906, Disc Loss: 1.4545, L1 Loss: 0.0637\n","Epoch [33/50], Step [40/40], Gen. Loss: 1.4270, Disc Loss: 1.3841, L1 Loss: 0.0563\n","Epoch [34/50], Step [40/40], Gen. Loss: 1.4354, Disc Loss: 1.3721, L1 Loss: 0.0555\n","Epoch [35/50], Step [40/40], Gen. Loss: 1.4625, Disc Loss: 1.3460, L1 Loss: 0.0544\n","Epoch [36/50], Step [40/40], Gen. Loss: 1.3886, Disc Loss: 1.4317, L1 Loss: 0.0614\n","Epoch [37/50], Step [40/40], Gen. Loss: 1.4194, Disc Loss: 1.3844, L1 Loss: 0.0542\n","Epoch [38/50], Step [40/40], Gen. Loss: 1.4341, Disc Loss: 1.3677, L1 Loss: 0.0538\n","Epoch [39/50], Step [40/40], Gen. Loss: 1.4528, Disc Loss: 1.3548, L1 Loss: 0.0544\n","Epoch [40/50], Step [40/40], Gen. Loss: 1.4142, Disc Loss: 1.4140, L1 Loss: 0.0607\n","Epoch [41/50], Step [40/40], Gen. Loss: 1.4773, Disc Loss: 1.3509, L1 Loss: 0.0553\n","Epoch [42/50], Step [40/40], Gen. Loss: 1.4258, Disc Loss: 1.4160, L1 Loss: 0.0636\n","Epoch [43/50], Step [40/40], Gen. Loss: 1.4197, Disc Loss: 1.4071, L1 Loss: 0.0621\n","Epoch [44/50], Step [40/40], Gen. Loss: 1.4114, Disc Loss: 1.4223, L1 Loss: 0.0608\n","Epoch [45/50], Step [40/40], Gen. Loss: 1.4068, Disc Loss: 1.4032, L1 Loss: 0.0554\n","Epoch [46/50], Step [40/40], Gen. Loss: 1.4352, Disc Loss: 1.3642, L1 Loss: 0.0537\n","Epoch [47/50], Step [40/40], Gen. Loss: 1.4300, Disc Loss: 1.3760, L1 Loss: 0.0551\n","Epoch [48/50], Step [40/40], Gen. Loss: 1.4304, Disc Loss: 1.3729, L1 Loss: 0.0548\n","Epoch [49/50], Step [40/40], Gen. Loss: 1.4073, Disc Loss: 1.4002, L1 Loss: 0.0588\n","Epoch [50/50], Step [40/40], Gen. Loss: 1.4528, Disc Loss: 1.3725, L1 Loss: 0.0544\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rg853bsit5so","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LFmwbaVaJf4JQ8sVIp9QRFKsjFrSeh2c"},"executionInfo":{"status":"ok","timestamp":1624203262094,"user_tz":-120,"elapsed":6642,"user":{"displayName":"MARTÍ COMA","photoUrl":"","userId":"01624721078849938893"}},"outputId":"6bd4a6cb-4b59-4c90-ccbd-b88420d08cb2"},"source":["def superResolution():\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    # Load generator\n","    gan_gen = Generator()\n","    gan_gen = gan_gen.to(device)\n","    gan_gen.load_state_dict(torch.load(results_path+'gan_ESRGAN.ckpt'))\n","    gan_gen.eval() # Put in eval model\n","    gan_gen = gan_gen.to(device)\n","\n","    reconstruction_error_avg = 0\n","    n_batches = 0\n","    # Init a Disc\n","    gan_disc = Discriminator(1)\n","    gan_disc = gan_disc.to(device)\n","  \n","    # Load test dataset\n","    test_loader = torch.utils.data.DataLoader(dataset=STLTest,\n","                                               batch_size=1, \n","                                               shuffle=True)\n","\n","    for i, (real_images,_) in enumerate(test_loader):\n","      real_images = real_images.to(device)\n","      if i >= 10: break\n","      x_gen = gan_gen.forward(gan_gen.sample(real_images))\n","      plt.imshow(np.moveaxis(x_gen.cpu().squeeze().detach().numpy(), 0, 2))\n","      plt.axis('off')\n","      plt.title(\"Generated Image low resolution\")\n","      plt.show()\n","\n","      plt.imshow(np.moveaxis(gan_gen.sample(real_images).cpu().squeeze().detach().numpy(), 0, 2))\n","      plt.axis('off')\n","      plt.title(\"Real Image low resolution\")\n","      plt.show()\n","\n","      x_gen_high_res = gan_gen.forward(real_images)\n","      plt.imshow(np.moveaxis(x_gen_high_res.cpu().squeeze().detach().numpy(), 0, 2))\n","      plt.axis('off')\n","      plt.title(\"Generated Image high resolution\")\n","      plt.show()\n","\n","      plt.imshow(np.moveaxis(real_images.cpu().squeeze().detach().numpy(), 0, 2))\n","      plt.axis('off')\n","      plt.title(\"Real Image high resolution\")\n","      plt.show()\n","\n","      reconstruction_error = (real_images-x_gen).abs().mean()\n","\n","      reconstruction_error_avg += reconstruction_error.cpu().item()\n","      n_batches+=1\n","\n","    print(\"Reconstruction error: \", str(reconstruction_error_avg/n_batches))\n","\n","\n","superResolution()"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"QD1YqSTpvYve"},"source":[""],"execution_count":null,"outputs":[]}]}